{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    COMP4240/5435 - Reinforcement Learning\n",
    "\n",
    "# Homework 3 - Dynamic Programming\n",
    "\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: _____Harshal Dafade_____ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to study different properties of dynamic programming methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are allowed to use the following modules\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- Avoid including magical numbers in your code. Instead, define constants and use meaningful variable names.\n",
    "- In python, you can pass a function as another function's argument.\n",
    "- It would be very useful to test each function thouroughly and making sure it does what it is supposed to do, before moving to the next step.\n",
    "  \n",
    "**General Notes:**\n",
    "- Questions marked with * are optional for COMP4240 - Undergraduate section. Questions marked as extra credit are optional for everyone.\n",
    "- Do not use a mix of python lists and numpy arrays. Every vector or matrix in your code should be a numpy array. \n",
    "- For functions that exist in both the python core and the numpy library, use the one in the numpy library. For example, use `np.max` instead of `max`. Another example: use `np.random.normal` instead of `random.gauss`.\n",
    "- Make sure all of your plots have a proper size and include `xlabel`, `ylabel`, `legend`, `title`, and `grid`."
   ]
  },
  {
   "attachments": {
    "g1313.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Deterministic Clearnig Robot [40 pts]\n",
    "Consider a cleaning robot that must collect an empty can and also has to recharge its batteries.\n",
    "![g1313.png](attachment:g1313.png)\n",
    "\n",
    "This problem has a discrete state space $S=\\{0,…,9\\}$, where state $s$ describes the position of the robot in the corridor. The robot has only two actions $A=\\{-1,1\\}$ for going one step to the left or right. States $0$ and $9$ are terminal, meaning that once the robot reaches either of them it can no longer leave, regardless of the action, and the episode ends. We assume this is a deterministic environment with $\\gamma=0.9$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1 [5 pts]\n",
    "Write a function that describes the transition function $s'=T(s,a)$ and test it for a few state-action pairs including terminal states. This is a deterministic transition function meaning given the current state and action, it should return the next state. Note that the transition function describes robot’s environment model and should not allow the robot to move outside the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_transition(s, a):\n",
    "    # this function should return next state\n",
    "    #--- Your code here ---#\n",
    "    if s == 0 or s == 9: \n",
    "        return s\n",
    "    next_state = s + a\n",
    "\n",
    "    return max(0, min(9, next_state))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2 [5 pts]\n",
    "Write a reward function $R(s,a)$ that gives a reward of $+5$ for being at $s=8$ and taking action $a=1$ ; a reward $+1$ for being at $s=1$ and taking action $a=-1$ ; and reward $0$ otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_for_deterministic_robot(s, a):\n",
    "    # this function should return a scalar reward\n",
    "    #--- Your code here ---#\n",
    "    if s == 8 and a == 1:\n",
    "        return 5\n",
    "    elif s == 1 and a == -1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3 [10 pts]\n",
    "Implement the value iteration algorithm to find $V^*(s)$ and $π^*(s)$. Note that instead of using the dynamics of the environment, $p(s',r|s,a)$, you should use the deterministic transition function $T(s,a)$ that allows you to remove looping over possible rewards and the probability of transitions (because it is a deterministic function). Print out the final $V^*(s)$ and $π^*(s)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_deterministic(transition_fcn, reward_fcn, V0, gamma=0.9, theta=1e-6):\n",
    "    # This function should run multiple iteration and return V (state-value) and P (policy)\n",
    "    # you might need to add more arguments to this function as input\n",
    "\n",
    "    V = np.copy(V0)\n",
    "    P = np.zeros(len(V), dtype=int)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(len(V)):\n",
    "            if s == 0 or s == 9:\n",
    "                continue\n",
    "            \n",
    "            v = V[s]\n",
    "            action_values = []\n",
    "            for a in [-1, 1]:\n",
    "                next_state = transition_fcn(s, a)\n",
    "                action_values.append(reward_fcn(s, a) + gamma * V[next_state])\n",
    "    # 1- Calculate V\n",
    "    #--- Your code here ---#\n",
    "            V[s] = max(action_values)\n",
    "    # 2- Calculate P\n",
    "    #--- Your code here ---#\n",
    "            P[s] = -1 if action_values[0] > action_values[1] else 1\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "    return V, P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4 [10 pts]\n",
    "Write a function that calls the value iteration algorithm you implemented in previous step. Print the final V and P. Do the calculated V and P make sense for the deterministic robot scenario? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final V*(s): [0.        2.3914845 2.657205  2.95245   3.2805    3.645     4.05\n",
      " 4.5       5.        0.       ]\n",
      "Final π*(s): [0 1 1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "def deterministic_cleaning_robot():\n",
    "    # This function should call the value iteration algorithm\n",
    "    #--- Your code here ---#\n",
    "    V0 = np.zeros(10) \n",
    "    V, P = value_iteration_deterministic(deterministic_transition, reward_for_deterministic_robot, V0)\n",
    "    \n",
    "    print(\"Final V*(s):\", V)\n",
    "    print(\"Final π*(s):\", P)\n",
    "\n",
    "deterministic_cleaning_robot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer\n",
    "\n",
    "The final value function V*(s) clearly shows values for terminal states to be 0, this is expected as the robot cannot earn any rewards from these states(0 and 9). The states between 1 and 8 show increasing values as the robot's ability to accrue rewards increases as it approaches state 8, where it obtains the maximum reward of +5 for making the transition. The policy π*(s) shows that moving right at all states between 1 to 8 is optimal action which is indicated by policy value of 1. This is rational as it helps the robot move towards state 8 which yeild max reward. Again at terminal states the policy remains zero as there is no need to take further actions when in terminal states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Stochastic Cleaning Robot [60 pts]\n",
    "Consider again the cleaning robot in the previous section. Assume that, due to uncertainties in the environment, such as slippery floor, state transitions are no longer deterministic. When trying to move in a certain direction, the robot succeeds with a probability of 0.8. With a probability of 0.15 it remains in the same state, and it may even move in the opposite direction with a probability of 0.05. Remember that the robot still only has two actions $A=\\{ -1, 1\\}$ but now the action execution is not deterministic anymore. We also assume $\\gamma=0.9$ for this stochastic environment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 [10 pts]\n",
    "Write a function that describes the stochastic transition function. Given the current state and action, this function should return the robot's next state by considering the stochasticities described above. Note that the transition function describes robot’s environment model and should not allow the robot to move outside the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_transition(s, a):\n",
    "    # this function should return next state\n",
    "    #--- Your code here ---#\n",
    "    if s == 0 or s == 9: \n",
    "        return s\n",
    "\n",
    "    next_states = [0] * 10 \n",
    "    if a == 1: \n",
    "        next_states[s] += 0.15 \n",
    "        if s + 1 <= 9:\n",
    "            next_states[s + 1] += 0.8 \n",
    "        if s - 1 >= 0:\n",
    "            next_states[s - 1] += 0.05\n",
    "    elif a == -1: \n",
    "        next_states[s] += 0.15 \n",
    "        if s - 1 >= 0:\n",
    "            next_states[s - 1] += 0.8 \n",
    "        if s + 1 <= 9:\n",
    "            next_states[s + 1] += 0.05 \n",
    "\n",
    "    return np.random.choice(range(10), p=next_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 [10 pts]\n",
    "Write a reward function $R(s, a, s')$ that gives a reward of $+5$ for not being at $s=9$ and taking some action, ending up in that terminal state; a reward $+1$ for being at $s=0$ and taking any action and ending up in that terminal state; and reward $0$ otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_for_stochastic_robot(s, a, sp):\n",
    "    # this function should return a scalar reward\n",
    "    # the inclusion of action in the arguments might be unnecessary\n",
    "    #--- Your code here ---#\n",
    "    if sp == 9: \n",
    "        return 5 if s != 9 else 0\n",
    "    elif s == 0:\n",
    "        return 1 if sp == 9 else 0\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3 [10 pts]\n",
    "To implement the stochastic version of the value iteration algorithm, you would need the dynamic model of the environment $p(s',r |s,a)$. However, since the reward is deterministic, you only need a simpler form of the dynamic model that only includes probabilities of the next state $p(s'|s,a)$. Write a function that generates the probabilities. You can return an array that includes all the probabilities. It would be very helpful if you try to write the table on paper before coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_probability_table():\n",
    "    #--- Your code here ---#\n",
    "    P = np.zeros((10, 2, 10)) \n",
    "    \n",
    "    for s in range(10):\n",
    "        for a in [-1, 1]: \n",
    "            if s == 0:  \n",
    "                P[s][0][s] = 1.0\n",
    "                continue\n",
    "            if s == 9: \n",
    "                P[s][1][s] = 1.0\n",
    "                continue\n",
    "            \n",
    "            if a == 1:  \n",
    "                P[s][1][s] += 0.15  \n",
    "                if s + 1 <= 9:\n",
    "                    P[s][1][s + 1] += 0.8  \n",
    "                if s - 1 >= 0:\n",
    "                    P[s][1][s - 1] += 0.05  \n",
    "            \n",
    "            elif a == -1:  \n",
    "                P[s][0][s] += 0.15\n",
    "                if s - 1 >= 0:\n",
    "                    P[s][0][s - 1] += 0.8  \n",
    "                if s + 1 <= 9:\n",
    "                    P[s][0][s + 1] += 0.05  \n",
    "                    \n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4 [20 pts]\n",
    "Implement the stochastic value iteration algorithm to find $V^*(s)$ and $π^*(s)$. Note that you should use the (reduced version of) dynamics of the environment, $p(s'|s,a)$, implemented in previous step. Print out the final $V^*(s)$ and $π^*(s)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_stochastic(transition_fcn, reward_fcn, V0, P_dynamics, gamma=0.9, theta=1e-6):\n",
    "    # This function should run multiple iteration and return V (state-value) and P (policy)\n",
    "    # you might need to add more arguments to this function as input\n",
    "    \n",
    "    V = np.copy(V0)\n",
    "    P = np.zeros(len(V), dtype=int)  \n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(len(V)):\n",
    "            if s == 0 or s == 9:  \n",
    "                continue\n",
    "            \n",
    "            v = V[s]\n",
    "            action_values = []\n",
    "            for a in [-1, 1]:\n",
    "                expected_value = 0\n",
    "                for sp in range(len(V)):\n",
    "                    expected_value += P_dynamics[s][0 if a == -1 else 1][sp] * (reward_fcn(s, a, sp) + gamma * V[sp])\n",
    "                action_values.append(expected_value)\n",
    "    # 1- Calculate V\n",
    "    #--- Your code here ---#           \n",
    "            V[s] = max(action_values)\n",
    "    # 2- Calculate P\n",
    "    #--- Your code here ---#\n",
    "            P[s] = -1 if action_values[0] > action_values[1] else 1\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "    return V, P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.5 [10 pts]\n",
    "Write a function that calls the stochastic value iteration algorithm you implemented in previous step. Print the final V and P. Do the calculated V and P make sense for the deterministic robot scenario? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final V*(s): [0.         1.76766328 2.12365139 2.44085232 2.79968466 3.21095678\n",
      " 3.6826275  4.22358296 4.84400143 0.        ]\n",
      "Final π*(s): [0 1 1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "def stochastic_cleaning_robot():\n",
    "    # This function should call the value iteration algorithm\n",
    "    #--- Your code here ---#\n",
    "    V0 = np.zeros(10)  \n",
    "    P_dynamics = generate_probability_table()\n",
    "    V, P = value_iteration_stochastic(stochastic_transition, reward_for_stochastic_robot, V0, P_dynamics)\n",
    "    \n",
    "    print(\"Final V*(s):\", V)\n",
    "    print(\"Final π*(s):\", P)\n",
    "\n",
    "stochastic_cleaning_robot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer\n",
    "\n",
    "The final value function V*(s) for terminal states is consistent with that of deterministic robot which turns out to be 0, as the robot cannot earn rewards from these states. However for states between 1 and 8 we see similar trend but values are lower than that of deterministic scenario. This happens because of the stochastic nature of our environment and the uncertainty of our robot's actions. In our specific case the robot has only a chance of 80% to move to our intended state, the result of which is reduced expected rewards. Despite all the uncertainty the policy π*(s) stays consistent directing the robot to move in the right direction in states between 1 and 8. The terminal states again show a policy value of 0 showing no further actions can be taken. Overall this result shows that uncertainty might lower the expected rewards but the optimal actions are clear and consistent as they guide the robot towards maximizing overall rewards.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
